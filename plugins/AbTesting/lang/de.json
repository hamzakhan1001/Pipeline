{
    "AbTesting": {
        "1DayAnd1Hour": "1 Tag und 1 Stunde",
        "1DayAndYHours": "1 Tag and %1$s Stunden",
        "1Hour": "1 Stunde",
        "AbTesting": "A/B-Tests",
        "ActionArchiveExperiment": "Archiviere A/B Test",
        "ActionArchiveExperimentSuccess": "A/B Test erfolgreich archiviert",
        "ActionEditExperimentAnyway": "Bearbeite der A/B Test dennoch",
        "ActionFinishExperiment": "Beende A/B Test",
        "ActionViewReport": "Gehe zum Report",
        "ActivateExperimentOnAllPages": "Besucher nehmen an diesem A/B Test teil, wenn diese irgendeine Seite besuchen",
        "ActiveExperimentOnSomePages": "Besucher nehmen nur an diesem A/B Test teil, wenn die URL entspricht",
        "ArchiveReportConfirm": "Wollen Sie wirklich diesen A/B Test archivieren? der A/B Test wird nicht mehr im Reporting-Menü angezeigt und wird nicht mehr für die Segmentierung zur Verfügung stehen, sobald es archiviert wurde.",
        "ArchiveReportInfo": "Archiviere A/B Test. Wenn Sie einen A/B Test archivieren, wird es nicht gelöscht, aber der A/B Test wird nicht mehr im Reporting-Menü erscheinen und nicht mehr für die Segmentierung verfügbar sein.",
        "AverageX": "Durchschnitt %1$s",
        "ChooseExperiment": "Wähle A/B Test",
        "ClickToCreateNewGoal": "Klicken Sie hier, um ein neues Ziel zu erstellen.",
        "ColumnBounceRateDocumentation": "Der Anteil der Besuche die nur eine Seite besuchten und am A/B Test teilgenommen haben. Dies bedeutet, dass der Besucher direkt eine Ziel-Seite des A/B Tests wieder verlassen hat, nachdem er dem A/B Test beigetreten ist.",
        "ColumnBouncesDocumentation": "Die Anzahl der Besuche die nur eine Seite besuchten und am A/B Test teilgenommen haben. Dies bedeutet, dass der Besucher direkt eine Ziel-Seite des A/B Tests wieder verlassen hat, nachdem er dem A/B Test beigetreten ist.",
        "ColumnConversionsPerVisit": "Konversionen pro Besuch",
        "ColumnDetectedEffect": "Erkannte Auswirkung",
        "ColumnDetectedEffectDocumentation": "Der Anteil der Veränderung verglichen mit der originalen Version. Ein positiver erkannter Effekt bedeutet, dass diese Variante besser funktioniert als die originale Version während eine negative erkannte Auswirkung bedeutet, dass die Variante schlechter funktioniert als die originale Version.",
        "ColumnOrdersPerVisit": "Bestellungen pro Besuch",
        "ColumnOrdersRevenuePerVisit": "Umsatz pro Besuch (von Bestellungen)",
        "ColumnRemainingVisitors": "Verbleibende Besucher",
        "ColumnRemainingVisitorsDocumentation": "Die Anzahl der zusätzlichen Besucher die benötigt werden, um ein bestandskräftiges Resultat zu schließen.",
        "ColumnRevenuePerVisit": "Umsatz pro Besuch",
        "ColumnSignificanceRate": "Statistische Signifikanz",
        "ColumnSignificanceRateDocumentation": "Je höher die statistische Signifikanz ist, umso höher ist die Wahrscheinlichkeit, dass die erzielte Auswirkung auch echt und wiederholbar ist und nicht zufällig erzielt wurde.",
        "ColumnTimeOnSite": "Zeit auf der Website",
        "ColumnTotalConversions": "Gesamtanzahl der Konversionen",
        "ColumnTotalConversionsPerVisit": "Konversionen pro Besuch",
        "ColumnTotalRevenue": "Gesamtumsatz",
        "ColumnTotalRevenuePerVisit": "Umsatz pro Besuch",
        "ColumnUniqueVisitorsDocumentation": "Die Anzahl der eindeutigen Besucher die an diesem A/B Test teilgenommen haben.",
        "ColumnVisitsDocumentation": "Die Anzahl der Besuche die am A/B Test teilgenommen haben. Alle Besuche die am A/B Test teilgenommenen haben zählen, daher ist diese Anzahl für gewöhnlich höher als die Anzahl der \"aktiv teil genommenen Besuche\".",
        "ColumnVisitsEnteredDocumentation": "Die Anzahl der Besuche wo der Besucher eine Ziel-Seite dieses A/B Tests besucht hat.",
        "ConclusionLosingVariation": "Es existiert eine Variante, die signifikant schlechter als die originale Version funktioniert.",
        "ConclusionNoConclusion": "Um ein bestandskräftiges Resultat zu schließen, darf eine Variante keine verbleibenden Besucher benötigen und es muss statistisch signifikant sein.",
        "ConclusionNoVariationHasEnoughVisitors": "Mehr Besucher werden benötigt, um ein bestandskräftiges Resultat zu schließen.",
        "ConclusionNoVariationIsSignificant": "Keine Variante hat derzeit genug Signifikanz um ein bestandskräftiges Resultat zu schließen.",
        "ConclusionNoVariationRecordedYet": "Bisher wurden noch keine A/B Test-Daten aufgezeichnet. Es erscheint, als hat noch kein Besucher am A/B Test teilgenommen. Vielleicht muss der Code für den A/B Test noch in Ihr Projekt eingebunden werden.",
        "ConclusionSignificantVariation": "Es existiert eine Variante, die signifikant besser als die originale Version funktioniert aber es übertrifft nicht die erwartete Minimal Erkannte Auswirkung von %1$s.",
        "ConclusionWinningVariation": "Es existiert eine Variante, die signifikant besser als die originale Version funktioniert und es übertrifft die erwartete Minimal Erkannte Auswirkung von %1$s.",
        "ConfidenceThreshold": "Vertrauensschwelle",
        "ConfirmFinishExperiment": "Wollen Sie den A/B Test wirklich beenden? Es wird nicht möglich sein den A/B Test erneut zu starten, sobald es beendet wurde. Vergessen Sie nicht den Code für diesen A/B Test von Ihrem Projekt zu entfernen damit Ihre Besucher nicht mehr am A/B Test teilnehmen.",
        "ConfirmUpdateStartsExperiment": "Das definierte Startdatum ist in der Vergangenheit. Dies bedeutet, dass der A/B Test sofort gestartet wird, wenn Sie fortfahren. Wollen Sie den A/B Test wirklich bereits starten? Es ist empfohlen einen A/B Test nicht zu verändern, sobald es gestartet wurde da es zu Fehlinterpretationen mit dem Ergebnis kommen könnte.",
        "CreateNewExperiment": "Neuen A/B Test erstellen",
        "CreateNewExperimentNow": "Erstelle nun einen neuen A/B Test",
        "CurrentTimeInUTC": "Die aktuelle Zeit in UTC ist",
        "Definition": "Definition",
        "DeleteExperimentConfirm": "Wollen Sie wirklich diesen A/B Test löschen? Sobald ein A/B Test gelöscht wurde, kann es nicht mehr wiederhergestellt werden.",
        "DeleteExperimentInfo": "Lösche A/B Test. Es wird nicht möglich sein den A/B Test wiederherzustellen.",
        "EcommerceOrders": "E-Commerce Bestellungen",
        "EcommerceOrdersRevenue": "E-Commerce Bestellungen Umsatz",
        "EditExperiment": "Bearbeite A/B Test %s",
        "EditThisExperiment": "Bearbeite A/B Test",
        "EmbedCode": "Code einbauen",
        "EqualsDateInYourTimezone": "Dies entspricht der folgenden Zeit in Ihrer Zeitzone:",
        "ErrorArrayMissingKey": "Fehlender array Schlüssel \"%1$s\" in \"%2$s\" an der Position \"%3$s\".",
        "ErrorArrayMissingValue": "Fehlender Wert für den Array-Schlüssel \"%1$s\" in \"%2$s\" bei Position \"%3$s\".",
        "ErrorCreateNoUrlDefined": "Bitte geben Sie eine URL an, um zu Bestimmen, auf welchen Seiten der A/B Test aktiviert werden soll.",
        "ErrorExperimentAlreadyStarted": "Der A/B Test kann nicht gestartet werden da es bereits gestartet wurde.",
        "ErrorExperimentCannotBeFinished": "Der A/B Test kann nicht beendet werden da es bereits beendet wurde.",
        "ErrorExperimentCannotBeUpdatedBecauseArchived": "Dieser A/B Test kann nicht aktualisiert werden da es bereits archiviert wurde.",
        "ErrorExperimentDoesNotExist": "Der angeforderte A/B Test existiert nicht",
        "ErrorExperimentNameIsAlreadyInUse": "Der Name des A/B Tests wird bereits von einem anderen A/B Test verwendet.",
        "ErrorInnerIsNotAnArray": "Jedes \"%1$s\" innerhalb von \"%2$s\" muss ein Array sein.",
        "ErrorInvalidRegExp": "Der reguläre Ausdruck \"%1$s\" hat kein gültiges Format.",
        "ErrorInvalidValue": "Ungültiger Wert für \"%1$s\" angegeben (\"%2$s\").",
        "ErrorNotAnArray": "\"%1$s\" muss ein Array sein.",
        "ErrorNotEnabledForExperiment": "Die angegebene \"%1$s\" ist nicht aktiviert für diesen A/B Test.",
        "ErrorNotValidUrl": "%1$s ist keine valide URL. Stellen Sie sicher, dass die URL zum Beispiel mit http:// beginnt.",
        "ErrorVariationAllocatedNot100Traffic": "Sie haben in der Summe mehr als 100%% zugewiesen. Bitte definieren Sie einen niedrigeren Wert für eine oder mehrere Varianten damit die Summe insgesamt 100%% ist.",
        "ErrorVariationAllocatedNotEnoughOriginal": "Die originale Version erhält wesentlich mehr Traffic als es standardmäßig würde. Es ist empfohlen den zugewiesenen Traffic für die originale Version zu erhöhen, indem Sie einen niedrigeren Anteil für die Varianten setzen.",
        "ErrorVariationNameOriginalNotAllowed": "Der Name \"Original\" ist ein reservierter Varianten-Name und kann nicht verwendet werden.",
        "ErrorXContainsOnlyNumbers": "Der \"%1$s\" darf nicht nur aus Zahlen bestehen, bitte verwenden Sie zumindest einen Buchstaben.",
        "ErrorXContainsWhitespace": "Der \"%1$s\" darf keine Leerzeichen enthalten.",
        "ErrorXLaterThanY": "\"%1$s\" ist später als \"%2$s\".",
        "ErrorXLaterThanYButEqual": "\"%1$s\" muss später sein als \"%2$s\" aber sie sind gleich.",
        "ErrorXNotANumber": "\"%1$s\" muss eine Nummer sein.",
        "ErrorXNotInFuture": "\"%1$s\" muss in der Zukunft sein.",
        "ErrorXNotProvided": "Bitte geben Sie einen Wert für \"%1$s\" an.",
        "ErrorXNotWhitelisted": "Der Wert für \"%1$s\" ist nicht erlaubt, verwenden Sie einen von: %2$s.",
        "ErrorXOnlyAlNumDash": "Sonderzeichen für \"%1$s\" sind nicht erlaubt.",
        "ErrorXTooHigh": "\"%1$s\" ist zu niedrig, der Wert darf maximal %2$s sein.",
        "ErrorXTooLong": "\"%1$s\" ist zu lang, maximal %2$s Zeichen sind erlaubt.",
        "ErrorXTooLow": "\"%1$s\" ist zu lang, der Wert muss mindestens %2$s sein.",
        "ExcludedTargets": "Ausgeschlossenes Ziel",
        "ExpectedImprovement": "Erwartete minimale erkannte Auswirkung",
        "Experiment": "A/B Test",
        "ExperimentCreated": "der A/B Test wurde erfolgreich erstellt. Nun können Sie es weiter konfigurieren. Es ist empfohlen einen Blick auf die \"Erfolgsmetriken\" zu werfen.",
        "ExperimentCreatedInfo1": "dieser A/B Test ist terminiert gestartet zu werden am",
        "ExperimentCreatedInfo2": "und wird laufen bis",
        "ExperimentCreatedInfo3": "Stellen Sie sicher alle Änderungen zu machen bevor der A/B Test startet. Es ist nicht empfohlen einen laufenden A/B Test zu verändern, sobald es gestartet wurde.",
        "ExperimentFinished": "der A/B Test wurde erfolgreich beendet.",
        "ExperimentFinishedInfo1": "dieser A/B Test ist beendet. Es ist empfohlen keine Änderungen vorzunehmen, wenn ein A/B Test beendet wurde, da dies zu Fehlinterpretationen im Ergebnis führen kann.",
        "ExperimentFinishedInfo2": "Stellen Sie sicher, zuvor eingebundenen Code für diesen A/B Test von Ihrer Website, Anwendung oder Server zu entfernen.",
        "ExperimentIsFinishedPleaseRemoveCode": "Da der A/B Test beendet wurde, stellen Sie sicher jeglichen Code für diesen A/B Test von Ihrer Website, Anwendung oder Server zu entfernen.",
        "ExperimentName": "A/B Test-Name",
        "ExperimentOverview": "A/B Test Übersicht",
        "ExperimentReportPreview": "Hier ist ein Beispiel eines Reportes, welcher angezeigt wird, sobald ein A/B Test gestartet wurde.",
        "ExperimentRequiresUpdateBeforeViewEmbedCode": "Sie haben nicht gespeicherte Änderungen für diesen A/B Test. Bitte speichern Sie zunächst den A/B Test oder brechen Sie die Bearbeitung ab um den Code für diesen A/B Test sehen zu können.",
        "ExperimentRunningInfo1": "dieser A/B Test startete am",
        "ExperimentRunningInfo2": "und ist terminiert beendet zu werden am",
        "ExperimentRunningInfo3": "Es ist empfohlen, keine Änderungen vorzunehmen, wenn ein A/B Test im Gange ist, da es dann zu Fehlinterpretationen mit dem Ergebnis kommen kann.",
        "ExperimentStarted": "der A/B Test wurde erfolgreich gestartet.",
        "ExperimentUpdated": "der A/B Test wurde erfolgreich aktualisiert.",
        "ExperimentWillStartFromFirstTrackingRequest": "Dieser A/B Test startet automatisch, sobald der A/B Test in Ihr Projekt eingebunden ist, es sei denn Sie haben ein Startdatum terminiert. Stellen Sie sicher alle benötigten Konfigurationen vorab zu machen, da es nicht empfohlen ist, einen A/B Test zu verändern, sobald es gestartet wurde.",
        "Experiments": "A/B-Tests",
        "ExperimentsOverviewSubcategoryHelp": "Mit einem A/B-Test können Sie verschiedene Versionen vergleichen und sehen, welche Variante erfolgreicher ist. Hier finden Sie eine Übersicht über alle aktiven Tests und die dazugehörigen Conversion Rates.",
        "FieldConfidenceThresholdHelp": "Das Ziel eines A/B Tests ist es, genug Daten zu sammeln um zuversichtlich Änderungen aufgrund des Ergebnisses des A/B Tests machen zu können. Je höher die ausgewählte Nummer, desto wahrscheinlicher ist es, dass das Ergebnis echt und wiederholbar ist und nicht zufällig erzielt wurde.",
        "FieldDescriptionHelp": "Dieses Feld kann verwendet werden, um zu Beschreiben, welche Dinge Sie in diesem A/B Test miteinander vergleichen. Zum Beispiel \"Vergleiche einen blauen mit einem roten Jetzt Kaufen Button farbe\".",
        "FieldDescriptionPlaceholder": "z.B. 'Vergleiche einen blauen mit einem roten Jetzt Kaufen Button farbe'",
        "FieldExcludedTargetsHelp": "Indem Sie bestimmte Seiten ausschließen, können Sie einschränken, auf welchen Seiten ein Besucher den A/B Test nicht beitreten wird. Wenn für eine Seite nur eine Bedingung davon zutrifft, wird der A/B Test für einen Besucher auf dieser Seite nicht aktiviert. Es müssen nicht alle Bedingungen zutreffen.",
        "FieldExcludedTargetsLabel": "Ein Besucher nimmt nicht am A/B Test teil wenn",
        "FieldExperimentNameHelp": "Der Name ist ein eindeutiger Name für diesen A/B Test. Der gewählte Name ist unter Umständen im Quellcode Ihres Projektes für Ihre Besucher sichtbar, wenn der A/B Test im Gange ist. Verwenden Sie lediglich Buchstaben und Zahlen ohne Leerzeichen oder Sonderzeichen. Maximal %1$s Zeichen sind erlaubt.",
        "FieldHypothesisHelp": "Die Hypothese definiert, was Sie von diesem A/B Test erwarten, was das Ergebnis sein wird und warum es das Ergebnis sein wird. Zum Beispiel \"%1$sWenn%2$s ich die Farbe des Jetzt Kaufen Buttons ändere, %3$sdann%4$s hoffe ich mehr Produkte zu verkaufen, %5$sweil%6$s der Button besser sichtbar sein wird.\". Die Hypothese ist ein wichtiger Schritt in der Definierung Ihres Experiments und es ist empfohlen sich die Zeit zu nehmen darüber nachzudenken.",
        "FieldHypothesisPlaceholder": "z.B. 'Wenn ich die Farbe des Jetzt Kaufen Buttons ändere, dann hoffe ich mehr Produkte zu verkaufen, weil der Button besser sichtbar sein wird.'",
        "FieldIncludedTargetsHelp2": "Mit Zielen können Sie festlegen, auf welchen Seiten ein Besucher in diesen A/B-Test aufgenommen werden soll. Sie können eine oder mehrere Bedingungen definieren. Sie können z. B. festlegen, dass ein A/B-Test immer dann ausgeführt wird, wenn die URL oder der Pfad einem bestimmten Wert entspricht oder nur, wenn ein bestimmter URL-Parameter in der URL vorhanden ist. Ein Besucher wird in den A/B-Test aufgenommen, wenn eine der Bedingungen erfüllt ist, nicht wenn alle Bedingungen erfüllt sind. Alle Bedingungen werden ohne Berücksichtigung der Groß-/Kleinschreibung ausgewertet.",
        "FieldIncludedTargetsLabel": "Ein Besucher nimmt am A/B Test teil wenn",
        "FieldMinimumDetectableEffectHelp1": "Die Minimum Erkannte Auswirkung ist die relative minimale Verbesserung die Sie von diesem A/B Test erwarten. Zum Beispiel, wenn die Konversionsrate eines Ziels derzeit bei 10%% liegt und Sie eine Auswirkung von 20%% erwarten, dann muss eine Variante mindestens eine Konversionsrate von 12%% vorweisen, um als siegreiche Variante zu gelten.",
        "FieldMinimumDetectableEffectHelp2": "Falls Sie durch diesen A/B Test eine kleine Auswirkung erwarten, ist es empfohlen 10%% auszuwählen, für eine mittlere Auswirkung 40%% und für eine große Auswirkung 70%%.",
        "FieldPercentageParticipantsHelp": "Definieren Sie wie viele Ihrer Besucher an diesem A/B Test teilnehmen sollen. Wenn Sie 70%% auswählen, dann werden 70%% aller Ihrer Besucher an diesem A/B Test teilnehmen und entweder die originale Version oder eine beliebige Variante davon sehen. Die anderen 30%% werden nicht am A/B Test teilnehmen und immer die originale Version sehen.",
        "FieldPercentageParticipantsLabel": "Anteil der Besucher die am A/B Test teilnehmen werden",
        "FieldPercentageVariationsHelp": "Es ist empfohlen jede Variante gleich oft zu aktivieren (Standard) aber Sie können den Anteil für jede Variante ändern. Die Summe aller Varianten inklusive der originalen Version sollte 100%% betragen. Stellen Sie sicher, dass ein Anteil des Traffics der originalen Version zugewiesen ist.",
        "FieldPercentageVariationsLabel": "Anteil Ihres Traffics der jeder Variante zugewiesen wird",
        "FieldRedirectHelp1": "Falls Sie Nutzer auf eine komplett andere Seite umleiten möchten, wenn diese an Ihrem A/B Test teilnehmen, können Sie optional eine Umleitungs-URL für jede Variante definieren. Dies ist nützlich, wenn Sie einen A/B Test im Browser mit unserer JavaScript A/B Test Bibliothek ausführen. Wenn Sie hier eine URL definieren, wird der Tracking-Code unter \"Code einbauen\" automatisch angepasst, sodass Sie den Tracking-Code einfach nur in Ihr Projekt einfügen müssen ohne jegliche Anpassung.",
        "FieldRedirectHelp2": "Falls die Umleitungen nicht auf allen Seiten ausgeführt werden sollen, stellen Sie bitter sicher, dass unter \"Ziel-Seiten\" definiert ist, wann eine Umleitung stattfinden soll.",
        "FieldRedirectHelp3": "Mit unserer %1$sPHP A/B Tests Bibliothek%2$s können Sie in Ihrem PHP-Projekt Ihre Nutzer auch serverseitig umleiten.",
        "FieldScheduleExperimentFinishHelp": "Lassen Sie das Feld leer, wenn Sie den A/B Test manuell beenden möchten. Wenn Sie ein Datum definieren, wird der A/B Test automatisch zu dieser Zeit beendet. Falls Sie ein Datum definieren, stellen Sie sicher, dass der A/B Test lange genug laufen wird damit die Ergebnisse echt und nicht zufällig erzielt werden. Das angegebene Datum wird in der Zeitzone %1$sUTC%2$s erwartet.",
        "FieldScheduleExperimentFinishLabel": "Beende A/B Test an",
        "FieldScheduleExperimentStartHelp": "Lassen Sie das Feld leer, wenn der A/B Test automatisch gestartet werden soll, sobald der A/B Test in Ihr Projekt eingebunden wurde. Stellen Sie sicher, dass der A/B Test vor dem definierten Startdatum komplett konfiguriert ist. Es ist nicht empfohlen einen A/B Test zu ändern, sobald es im Gange ist, da es zu Fehlinterpretationen führen kann. Das angegebene Datum wird in der Zeitzone %1$sUTC%2$s erwartet.",
        "FieldScheduleExperimentStartLabel": "Starte A/B Test an",
        "FieldSuccessConditionsHelp": "Wir verwenden den minimalen nachweisbaren Effekt und die Konfidenzschwelle (statistische Signifikanz), um die Anzahl der Besucher zu berechnen, die erforderlich ist, damit Sie sich auf die Ergebnisse verlassen können. Während der Laufzeit eines A/B-Tests können Sie viele verschiedene potenzielle Gewinner sehen, die den gewünschten Effekt erzielen. Sie müssen den A/B-Test jedoch lange genug laufen lassen, um sicherzustellen, dass der festgestellte Effekt nicht auf Zufälligkeiten zurückzuführen ist.",
        "FieldSuccessMetricsHelp1": "Erfolgsmetriken helfen Ihnen zu messen welche Varianten am erfolgreichsten sind und helfen Ihnen zu entscheiden, welche Variante in der Zukunft verwendet werden sollte.",
        "FieldSuccessMetricsHelp2": "Sie können eine oder mehrere Metriken auswählen, um Ihre Hypothese zu überprüfen. Matomo wird für jede ausgewählte Metrik einen Report anzeigen, damit Sie die verschiedenen Varianten miteinander vergleichen können.",
        "FieldSuccessMetricsHelp3": "Wir empfehlen, nach dem Start eines A/B-Tests keine der ausgewählten Erfolgsmetriken zu verändern, da dies zu Fehlinterpretationen der Ergebnisse führen kann.",
        "FieldSuccessMetricsLabel": "Wähle eine oder mehrere Erfolgsmetriken",
        "FieldVariationsHelp": "Variante ist die Bezeichnung für eine neue Version die Sie mit der originalen Version vergleichen. Wenn Sie zum Beispiel verschiedene Button-Farben miteinander vergleichen möchten, erstellen Sie eine Variante für jede Farbe die Sie vergleichen möchten. Verwenden Sie lediglich Buchstaben und Zahlen ohne Leerzeichen und Sonderzeichen. Maximal 50 Zeichen können für jede Variante verwendet werden.",
        "FilesystemDirectory": "verzeichnis",
        "Filter": "Filter",
        "FinishDate": "Enddatum",
        "FormCreateExperimentIntro": "Ein A/B Test ermöglicht es Ihnen, verschiedene Versionen miteinander zu vergleichen, um zu sehen, welche am besten funktioniert. Diese Felder werden benötigt, um einen A/B Test zu erstellen. Sobald der A/B Test erstellt wurde, können Sie es weiter an Ihre Bedürfnisse anpassen.",
        "FormScheduleIntroduction": "Standardmäßig startet ein A/B Test sobald der A/B Test in Ihr Projekt eingebunden ist und endet, sobald Sie es manuell beenden. Alternativ können Sie ein Start- und Enddatum terminieren.",
        "GettingStarted": "Erste Schritte",
        "HowToGetStartedAdminAccess": "Sie können jetzt %1$seinen neuen A/B-Test erstellen%2$s.",
        "HowToGetStartedUserAccess": "Bitten Sie einen Benutzer mit Schreibrechten, einen neuen A/B-Test unter dem Menüpunkt \"Administration\" zu erstellen.",
        "Hypothesis": "Hypothese",
        "IncludedTargets": "Inbegriffenes Ziel",
        "Manage": "Verwalten",
        "ManageExperiments": "A/B Test-Verwaltung",
        "ManageExperimentsIntroduction": "Ein A/B Test ermöglicht es Ihnen, verschiedene Versionen einer Website oder Anwendung miteinander zu vergleichen um herauszufinden, welche Version Sie am erfolgreichsten macht. A/B Tests sind auch bekannt als Experimente oder Split tests. In einem A/B Test erstellen Sie zwei oder mehrere Varianten für Ihre Besucher und die Variante die besser funktioniert, gewinnt. Wenn ein Besucher an einem A/B Test teilnimmt, wird eine der verschiedenen Varianten zufällig ausgewählt und der Besucher wird diese ausgewählte Variante für alle nachfolgenden Besuche sehen. Wenn Sie so experimentieren, maximieren Sie Ihren Erfolg.",
        "ManageExperimentsSubcategoryHelp": "Mit einem A/B-Test können Sie verschiedene Versionen vergleichen und feststellen, welche Version besser abschneidet. In diesem Abschnitt können Sie A/B-Tests für Ihre Website erstellen und verwalten.",
        "MenuTitleExperiment": "A/B Test \"%1$s\"",
        "MinimumDetectableEffectMDE": "Minimum Erkannte Auswirkung",
        "NExperiments": "%s A/B Tests",
        "NameOriginalVariation": "Original",
        "NavigationBack": "Zurück",
        "NeedHelp": "Hilfe benötigt?",
        "NewExperimentTargetPageHelp": "Standardmäßig wird ein A/B Test auf allen Ihren Seiten aktiviert. Alternativ können Sie einen A/B Test nur auf einer bestimmten Seite aktivieren. Falls Sie eine URL angeben, wird der A/B Test nur auf dieser Seite aktiviert. Sobald Sie den A/B Test erstellt haben, können Sie weitere Seiten ein- und ausschließen.",
        "NoActiveExperiment": "Es existiert kein A/B Test der derzeit im Gange oder beendet ist.",
        "NoActiveExperimentConfigured": "Es stehen keine aktiven A/B Tests zur Verfügung.",
        "NoExperimentsFound": "Es existieren keine A/B Tests mit diesem Status.",
        "PercentageParticipants": "Prozentualer Anteil der Teilnehmer",
        "PluginDescription": "A/B-Testing-Plugin",
        "Preview": "Muster-Report",
        "Redirects": "Umleitungen",
        "RelatedActions": "Aktionen, die hier ausgeführt werden können",
        "ReportDateCannotBeChanged": "Das Datum für einen A/B Test-Report kann nicht verändert werden",
        "ReportStatusFinished": "der A/B Test wurde beendet. Es lief für %1$s von %2$s bis %3$s.",
        "ReportStatusRunning": "der A/B Test ist im Gange für %1$s seit dem %2$s.",
        "ReportWhenToDeclareWinner": "Wann kann ein Gewinner ausgemacht werden? Ein A/B Test deutet möglicherweise eine gewinnende oder verlierende Variante an. Es ist jedoch entscheidend den A/B Test zumindest für ein oder zwei volle Geschäftsperioden laufen zu lassen. Das Verhalten von Benutzern verändert sich abhängig von der Tageszeit und dem Wochentag und es ist daher empfohlen A/B Tests für volle Tage oder besser volle Wochen laufen zu lassen, damit die gemessene Auswirkung nicht zufällig erzielt wurde.",
        "Rule": "Regel",
        "RunExperimentWithEmailCampaign": "Ausführen eines A/B Tests in einer Kampagne (z.B. einer Werbe-Kampagne oder E-Mail-Kampagne)",
        "RunExperimentWithJsClient": "Ausführen eines A/B Tests im Browser mit dem Matomo JavaScript Tracker",
        "RunExperimentWithJsTracker": "Ausführen eines A/B Tests für eine Website auf dem Server",
        "RunExperimentWithOtherSDK": "Ausführen eines A/B Tests in einer Anwendung auf iOS, Android, PHP, Java, C#, Python, …",
        "Schedule": "Terminieren",
        "StartDate": "Startdatum",
        "Status": "Status",
        "StatusActive": "Aktiv",
        "StatusArchived": "Archiviert",
        "StatusCreated": "Erstellt",
        "StatusFinished": "Beendet",
        "StatusRunning": "Im Gange",
        "SuccessConditions": "Erfolgsbedingungen",
        "SuccessMetric": "Erfolgsmetrik",
        "SuccessMetricDetails": "Erfolgsmetrik details",
        "SuccessMetrics": "Erfolgsmetriken",
        "Target": "Ziel",
        "TargetAttributePath": "Pfad",
        "TargetAttributeUrl": "URL",
        "TargetAttributeUrlParameter": "URL Parameter",
        "TargetAttributeUrlParameterExample": "nameEinesUrlParameters",
        "TargetComparisionsCaseInsensitive": "Alle Vergleiche ignorieren Groß- und Kleinschreibung.",
        "TargetComparisons": "Vergleiche",
        "TargetPageTestErrorInvalidUrl": "Geben Sie eine URL inklusive Schema an.",
        "TargetPageTestLabel": "Geben Sie eine URL inklusive Schema an um zu überprüfen, ob ein Besucher dem A/B Test beitreten wird, wenn ein Besucher diese URL aufruft:",
        "TargetPageTestTitle": "URL Prüfer",
        "TargetPageTestUrlMatches": "Ein Besucher wird diesen A/B Test bei dieser URL beitreten",
        "TargetPageTestUrlNotMatches": "Ein Besucher wird diesen A/B Test bei dieser URL nicht beitreten",
        "TargetPages": "Ziel-Seiten",
        "TargetTypeContains": "enthält",
        "TargetTypeEqualsExactly": "entspricht genau",
        "TargetTypeEqualsExactlyInfo": "Der Wert muss genau übereinstimmen, inklusive des URL-Schemas, der URL-Parameter und des URL-Hashes.",
        "TargetTypeEqualsSimple": "entspricht einfach",
        "TargetTypeEqualsSimpleInfo": "Das URL-Schema (z.B. http and https) und die subdomain \"www.\" muss nicht übereinstimmen und wird ignoriert. Ein abschließender Schrägstrich im Pfad wie auch URL-Parameter und URL-Hashes werden ebenso ignoriert, wenn die URL verglichen wird.",
        "TargetTypeExists": "existiert",
        "TargetTypeIsAny": "ist beliebig",
        "TargetTypeIsNot": "nicht %s",
        "TargetTypeRegExp": "entspricht dem Regulären Ausdruck",
        "TargetTypeRegExpInfo": "Beliebiger Regulärer Ausdruck, zum Beispiel \"^(.*)test(.*)$\".",
        "TargetTypeStartsWith": "startet mit",
        "TrafficAllocation": "Traffic Zuweisung",
        "UniqueVisitorsActivelyEntered": "Eindeutige Besucher aktiv teil genommen",
        "UpdatingData": "Aktualisiere Daten…",
        "UrlParameterValueToMatchPlaceholder": "Wert für den URL-Parameter",
        "Variation": "Variante",
        "VariationName": "Varianten-Name",
        "VariationPercentage": "Prozentsatz der Veränderung",
        "VariationRedirectUrl": "Varianten-URL",
        "Variations": "Varianten",
        "ViewReportInfo": "Gehe zum Report für diesen A/B Test.",
        "VisitEnteredExperiment": "Besuch hat am A/B Test teil genommen",
        "VisitorEnteredNExperiments": "Der Besucher hat %1$s A/B-Tests durchlaufen",
        "VisitorEnteredOneExperiment": "Der Besucher hat einen A/B-Test durchlaufen",
        "VisitsActivelyEntered": "Besuch aktiv teil genommen",
        "XDaysAnd1Hour": "%1$s Tage und 1 Stunde",
        "XDaysAndYHours": "%1$s Tage und %2$s Stunden",
        "Xhours": "%1$s Stunden"
    }
}
